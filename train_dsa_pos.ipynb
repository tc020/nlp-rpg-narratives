{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4535adf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Fine-Tuning auf DSA-Daten ...\n",
      "Epoche 1 - Loss: {'tok2vec': 161.5016918182373, 'tagger': 0.0, 'morphologizer': 53.240132331848145, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 2 - Loss: {'tok2vec': 30.115575637901202, 'tagger': 0.0, 'morphologizer': 8.026043093879707, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 3 - Loss: {'tok2vec': 4.158587197240422, 'tagger': 0.0, 'morphologizer': 0.7085235720361887, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 4 - Loss: {'tok2vec': 0.0064903936869644685, 'tagger': 0.0, 'morphologizer': 0.0011465696719241691, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 5 - Loss: {'tok2vec': 0.0003373495041086201, 'tagger': 0.0, 'morphologizer': 5.9781852585920336e-05, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 6 - Loss: {'tok2vec': 5.274267037289407e-06, 'tagger': 0.0, 'morphologizer': 1.4538145260327776e-06, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 7 - Loss: {'tok2vec': 1.5635855214832933e-06, 'tagger': 0.0, 'morphologizer': 5.611225729370943e-07, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 8 - Loss: {'tok2vec': 9.72927380540542e-07, 'tagger': 0.0, 'morphologizer': 3.661797575113014e-07, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 9 - Loss: {'tok2vec': 7.166543620970578e-07, 'tagger': 0.0, 'morphologizer': 2.7315879908961643e-07, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Epoche 10 - Loss: {'tok2vec': 5.675278419072644e-07, 'tagger': 0.0, 'morphologizer': 2.1730349821077674e-07, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': 0.0}\n",
      "Modell gespeichert unter: de_dsa_tagger\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from data.dsa_train_data_fixed import TRAIN_DATA\n",
    "\n",
    "# 1Ô∏è‚É£ Deutsches Basismodell laden\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# 2Ô∏è‚É£ POS-Tagger-Komponente holen\n",
    "tagger = nlp.get_pipe(\"tagger\")\n",
    "\n",
    "\"\"\"# 3Ô∏è‚É£ Falls du eigene Tags nutzt, hier hinzuf√ºgen (optional)\n",
    "for sent, ann in TRAIN_DATA:\n",
    "    for tag in ann[\"pos\"]:\n",
    "        if tag not in tagger.labels:\n",
    "            tagger.add_label(tag)\"\"\"\n",
    "\n",
    "# 4Ô∏è‚É£ Fine-Tuning\n",
    "print(\"Starte Fine-Tuning auf DSA-Daten ...\")\n",
    "for epoch in range(10):\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses)\n",
    "    print(f\"Epoche {epoch+1} - Loss: {losses}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Modell speichern\n",
    "output_dir = \"de_dsa_tagger\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Modell gespeichert unter: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddd5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Tokenanzahl passt nicht: 'Der Perainegeweihte heilt die Wunden des Bauern.'\n",
      "  Tokens: ['Der', 'Perainegeweihte', 'heilt', 'die', 'Wunden', 'des', 'Bauern', '.']\n",
      "  POS:    ['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from data.dsa_train_data import TRAIN_DATA\n",
    "\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "for text, ann in TRAIN_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    if len(doc) != len(ann[\"pos\"]):\n",
    "        print(f\"‚ö†Ô∏è  Tokenanzahl passt nicht: '{text}'\")\n",
    "        print(f\"  Tokens: {[t.text for t in doc]}\")\n",
    "        print(f\"  POS:    {ann['pos']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08212534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç √úberpr√ºfe DSA-Trainingsdaten auf Token/POS-L√§ngen...\n",
      "\n",
      "‚ö†Ô∏è  Mismatch in Satz: Der Perainegeweihte heilt die Wunden des Bauern.\n",
      "   Tokens (8): ['Der', 'Perainegeweihte', 'heilt', 'die', 'Wunden', 'des', 'Bauern', '.']\n",
      "   POS    (9): ['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "üëâ Vorschlag korrigiert (8): ['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN']\n",
      "\n",
      "\n",
      "‚úÖ √úberpr√ºfung abgeschlossen. 1 fehlerhafte S√§tze gefunden.\n",
      "\n",
      "üìÅ Gespeichert als: data/dsa_train_data_fixed.py\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from data.dsa_train_data import TRAIN_DATA\n",
    "from pprint import pprint\n",
    "\n",
    "# ‚öôÔ∏è Lade ein deutsches Tokenizer-Modell (nur zum Tokenisieren!)\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "print(\"üîç √úberpr√ºfe DSA-Trainingsdaten auf Token/POS-L√§ngen...\\n\")\n",
    "\n",
    "fixed_data = []\n",
    "errors = 0\n",
    "\n",
    "for text, ann in TRAIN_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    tokens = [t.text for t in doc]\n",
    "    pos = ann[\"pos\"]\n",
    "\n",
    "    if len(tokens) != len(pos):\n",
    "        errors += 1\n",
    "        print(f\"‚ö†Ô∏è  Mismatch in Satz: {text}\")\n",
    "        print(f\"   Tokens ({len(tokens)}): {tokens}\")\n",
    "        print(f\"   POS    ({len(pos)}): {pos}\")\n",
    "\n",
    "        # Vorschlag: gleiche L√§nge auff√ºllen\n",
    "        # Wenn mehr Tokens ‚Üí k√ºrze POS\n",
    "        # Wenn weniger Tokens ‚Üí f√ºge 'X' hinzu (unbekannt)\n",
    "        if len(tokens) > len(pos):\n",
    "            corrected_pos = pos + [\"X\"] * (len(tokens) - len(pos))\n",
    "        else:\n",
    "            corrected_pos = pos[:len(tokens)]\n",
    "\n",
    "        print(f\"üëâ Vorschlag korrigiert ({len(corrected_pos)}): {corrected_pos}\\n\")\n",
    "\n",
    "        fixed_data.append((text, {\"pos\": corrected_pos}))\n",
    "    else:\n",
    "        fixed_data.append((text, ann))\n",
    "\n",
    "print(f\"\\n‚úÖ √úberpr√ºfung abgeschlossen. {errors} fehlerhafte S√§tze gefunden.\\n\")\n",
    "\n",
    "# Optional: korrigierte Datei speichern\n",
    "save_choice = input(\"üíæ Korrigierte Daten als neue Datei speichern? (j/n): \").strip().lower()\n",
    "if save_choice == \"j\":\n",
    "    with open(\"data/dsa_train_data_fixed.py\", \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"TRAIN_DATA = [\\n\")\n",
    "        for text, ann in fixed_data:\n",
    "            f.write(f'    ({text!r}, {ann!r}),\\n')\n",
    "        f.write(\"]\\n\")\n",
    "    print(\"üìÅ Gespeichert als: data/dsa_train_data_fixed.py\")\n",
    "else:\n",
    "    print(\"‚ùå Keine Datei gespeichert. Nur √ºberpr√ºft.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f101c38",
   "metadata": {},
   "source": [
    "### Wenn du eigene neue Tags brauchst (z. B. ‚ÄûARTEFAKT‚Äú, ‚ÄûGOTT‚Äú usw.)\n",
    "\n",
    "Dann kannst du nicht das fertige Modell de_core_news_sm nehmen,\n",
    "sondern musst eine leere Pipeline mit neuem Tagger aufbauen, z. B.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8592b7d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E971] Found incompatible lengths in `Doc.from_array`: 9 for the array and 8 for the Doc itself.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text, annotations \u001b[38;5;129;01min\u001b[39;00m TRAIN_DATA:\n\u001b[32m     22\u001b[39m     doc = nlp.make_doc(text)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     example = \u001b[43mExample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     nlp.update([example], losses=losses)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoche \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/training/example.pyx:130\u001b[39m, in \u001b[36mspacy.training.example.Example.from_dict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/training/example.pyx:38\u001b[39m, in \u001b[36mspacy.training.example.annotations_to_doc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/tokens/doc.pyx:1099\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.from_array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: [E971] Found incompatible lengths in `Doc.from_array`: 9 for the array and 8 for the Doc itself."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import Tagger\n",
    "from spacy.training import Example\n",
    "from data.dsa_train_data import TRAIN_DATA\n",
    "\n",
    "# 1Ô∏è‚É£ Leere deutsche Pipeline erstellen\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "# 2Ô∏è‚É£ Neuen Tagger hinzuf√ºgen\n",
    "tagger = nlp.add_pipe(\"tagger\")\n",
    "\n",
    "# 3Ô∏è‚É£ Alle POS-Tags hinzuf√ºgen\n",
    "for _, ann in TRAIN_DATA:\n",
    "    for tag in ann[\"pos\"]:\n",
    "        tagger.add_label(tag)\n",
    "\n",
    "# 4Ô∏è‚É£ Training starten\n",
    "nlp.begin_training()\n",
    "for epoch in range(20):\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses)\n",
    "    print(f\"Epoche {epoch+1} - Loss: {losses}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Modell speichern\n",
    "#nlp.to_disk(\"de_dsa_tagger_custom\")\n",
    "print(\"‚úÖ Neues DSA-Tagger-Modell gespeichert!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb2643",
   "metadata": {},
   "source": [
    "## Evaluate DSA POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480c8d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_dsa_tagger'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Lade beide Modelle\u001b[39;00m\n\u001b[32m      5\u001b[39m nlp_base = spacy.load(\u001b[33m\"\u001b[39m\u001b[33mde_core_news_sm\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m nlp_dsa  = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mde_dsa_tagger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Vergleiche Basismodell vs. DSA-Modell\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Hilfsfunktion f√ºr Genauigkeit\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'de_dsa_tagger'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from data.dsa_train_data import TRAIN_DATA\n",
    "\n",
    "# Lade beide Modelle\n",
    "nlp_base = spacy.load(\"de_core_news_sm\")\n",
    "nlp_dsa  = spacy.load(\"de_dsa_tagger\")\n",
    "\n",
    "print(\"üîç Vergleiche Basismodell vs. DSA-Modell\\n\")\n",
    "\n",
    "# Hilfsfunktion f√ºr Genauigkeit\n",
    "def evaluate(model, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for text, annotations in data:\n",
    "        doc = model(text)\n",
    "        predicted_tags = [token.pos_ for token in doc]\n",
    "        gold_tags = annotations[\"pos\"]\n",
    "        total += len(gold_tags)\n",
    "        correct += sum(p == g for p, g in zip(predicted_tags, gold_tags))\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# 1Ô∏è‚É£ Genauigkeit berechnen\n",
    "acc_base = evaluate(nlp_base, TRAIN_DATA)\n",
    "acc_dsa  = evaluate(nlp_dsa,  TRAIN_DATA)\n",
    "\n",
    "print(f\"Basismodell: {acc_base*100:.2f}% korrekt\")\n",
    "print(f\"DSA-Modell : {acc_dsa*100:.2f}% korrekt\\n\")\n",
    "\n",
    "# 2Ô∏è‚É£ Zeige Unterschiede\n",
    "print(\"‚öîÔ∏è  Unterschiede zwischen den Modellen:\")\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    doc_base = nlp_base(text)\n",
    "    doc_dsa  = nlp_dsa(text)\n",
    "    gold_tags = annotations[\"pos\"]\n",
    "    print(f\"\\nüìú Satz: {text}\")\n",
    "    for token, gold, pred_b, pred_d in zip(doc_dsa, gold_tags,\n",
    "                                           [t.pos_ for t in doc_base],\n",
    "                                           [t.pos_ for t in doc_dsa]):\n",
    "        mark = \"‚úÖ\" if pred_d == gold else \"‚ùå\"\n",
    "        diff = \"\" if pred_b == pred_d else f\"(Base:{pred_b}‚ÜíDSA:{pred_d})\"\n",
    "        print(f\"  {token.text:12} Gold:{gold:6} DSA:{pred_d:6} {mark} {diff}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rpg-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
