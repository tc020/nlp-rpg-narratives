{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dac3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa_dnames.txt       # Dämonennamen\n",
    "dsa_gnames.txt       # Gottheiten\n",
    "dsa_names.txt        # Aventurische Personennamen\n",
    "dsa_stadtnames.txt   # Städte Aventuriens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b6459",
   "metadata": {},
   "source": [
    "NER Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6670 person names, 566 cities.\n",
      "Generated 7577 training examples.\n",
      "Iteration 1/20 - Losses: {'ner': np.float32(1693.5444)}\n",
      "Iteration 2/20 - Losses: {'ner': np.float32(1348.3699)}\n",
      "Iteration 3/20 - Losses: {'ner': np.float32(977.12573)}\n",
      "Iteration 4/20 - Losses: {'ner': np.float32(760.9753)}\n",
      "Iteration 5/20 - Losses: {'ner': np.float32(643.8173)}\n",
      "Iteration 6/20 - Losses: {'ner': np.float32(569.2016)}\n",
      "Iteration 7/20 - Losses: {'ner': np.float32(437.60535)}\n",
      "Iteration 8/20 - Losses: {'ner': np.float32(389.65405)}\n",
      "Iteration 9/20 - Losses: {'ner': np.float32(368.53146)}\n",
      "Iteration 10/20 - Losses: {'ner': np.float32(339.902)}\n",
      "Iteration 11/20 - Losses: {'ner': np.float32(319.56378)}\n",
      "Iteration 12/20 - Losses: {'ner': np.float32(291.34076)}\n",
      "Iteration 13/20 - Losses: {'ner': np.float32(261.15192)}\n",
      "Iteration 14/20 - Losses: {'ner': np.float32(236.98045)}\n",
      "Iteration 15/20 - Losses: {'ner': np.float32(277.1692)}\n",
      "Iteration 16/20 - Losses: {'ner': np.float32(248.64323)}\n",
      "Iteration 17/20 - Losses: {'ner': np.float32(243.76578)}\n",
      "Iteration 18/20 - Losses: {'ner': np.float32(275.49677)}\n",
      "Iteration 19/20 - Losses: {'ner': np.float32(280.02896)}\n",
      "Iteration 20/20 - Losses: {'ner': np.float32(261.01593)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# 5. Modell speichern\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"output_dir = Path(\"dsa_spacy_model\")\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03moutput_dir.mkdir(exist_ok=True)\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03mnlp.to_disk(output_dir)\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moutput_dir\u001b[49m.absolute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "#Dateien laden\n",
    "def load_list(path):\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "demon_names = load_list(\"dsa_dnames.txt\")\n",
    "god_names = load_list(\"dsa_gnames.txt\")\n",
    "person_names = load_list(\"dsa_names.txt\")\n",
    "city_names = load_list(\"dsa_stadtnames.txt\")\n",
    "\n",
    "print(f\"Loaded {len(person_names)} person names, {len(city_names)} cities.\")\n",
    "\n",
    "#Aus Listen einfache Trainingsdaten generieren\n",
    "def make_training_samples(words, label):\n",
    "    samples = []\n",
    "    for w in words:\n",
    "        sent = f\"{w} erschien plötzlich in der Geschichte.\"\n",
    "        start = sent.index(w)\n",
    "        end = start + len(w)\n",
    "        samples.append((sent, {\"entities\": [(start, end, label)]}))\n",
    "    return samples\n",
    "\n",
    "train_data = []\n",
    "train_data += make_training_samples(person_names, \"PER\")\n",
    "train_data += make_training_samples(city_names, \"LOC\")\n",
    "train_data += make_training_samples(demon_names, \"PER\")   # oder custom label?\n",
    "train_data += make_training_samples(god_names, \"PER\")     # ggf. \"MISC\" oder \"LOC\"?\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "print(f\"Generated {len(train_data)} training examples.\")\n",
    "\n",
    "#spaCy-Modell laden und NER-Komponente vorbereiten\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Labels hinzufügen\n",
    "ner.add_label(\"PER\")\n",
    "ner.add_label(\"LOC\")\n",
    "\n",
    "#Training\n",
    "nlp.disable_pipes([p for p in nlp.pipe_names if p != \"ner\"])\n",
    "\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "n_iters = 20\n",
    "\n",
    "for it in range(n_iters):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    for text, annotations in train_data:\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "        nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(f\"Iteration {it+1}/{n_iters} - Losses: {losses}\")\n",
    "\n",
    "#Modell speichern\n",
    "\"\"\"output_dir = Path(\"dsa_spacy_model\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "nlp.to_disk(output_dir)\"\"\"\n",
    "\n",
    "#print(f\"Model saved to {output_dir.absolute()}\")\n",
    "\n",
    "print(\"Training abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee0e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "Bei der LOC\n",
      "ihre Haut wirklich so dunkel sei muss Amara lachen. \"Ja, die ist schon immer so.\" antwortet sie und reicht der Geweihten dann ihre Hand, damit diese sich die dunkle Haut näher PER\n",
      "\"In Thorwal PER\n",
      "wir die Nummer natürlich nicht. Aber Ihr habt natürlich recht. Es PER\n",
      "eine Provokation LOC\n",
      ". Früher hatten wir das auch nicht drin. Es hat sich eigentlich PER\n",
      "eher zufällig so ergeben. Das Publikum reagiert eben einfach stärker auf eine kaum zu bändigende Wilde als auf eine ... Naja, normale Artistin die zufällig dunkle Haut hat. Ein mal hatten wir einen tapferen Recken im Publikum, der noch während der Vorstellung in die Manege gestürmt ist um mich zu befreien. Aber wir sind bei der Nummer ja hinter Gittern und ehe er da rein gekommen PER\n",
      "Vorhang wo wir PER\n",
      "bin unhöflich. Setzt Euch doch. Darf ich Euch etwas zu trinken anbieten?\" PER\n"
     ]
    }
   ],
   "source": [
    "test_text_real = 'Bei der Frage ob ihre Haut wirklich so dunkel sei muss Amara lachen. \"Ja, die ist schon immer so.\" \\\n",
    "antwortet sie und reicht der Geweihten dann ihre Hand, damit diese sich die dunkle Haut näher \\\n",
    "ansehen kann. Offenbar ist sie durchaus daran gewöhnt solche Fragen zu bekommen. \\\n",
    "Unterdessen antwortet sie aber schon auf die übrigen Bemerkungen. \\\n",
    "\"In Thorwal bringen wir die Nummer natürlich nicht. Aber Ihr habt natürlich recht. Es ist eine \\\n",
    "Provokation. Früher hatten wir das auch nicht drin. Es hat sich eigentlich eher zufällig so ergeben. \\\n",
    "Das Publikum reagiert eben einfach stärker auf eine kaum zu bändigende Wilde als auf eine ... \\\n",
    "Naja, normale Artistin die zufällig dunkle Haut hat. \\\n",
    "Ein mal hatten wir einen tapferen Recken im Publikum, der noch während der Vorstellung in die \\\n",
    "Manege gestürmt ist um mich zu befreien. Aber wir sind bei der Nummer ja hinter Gittern und ehe \\\n",
    "er da rein gekommen ist war ich schon hinter dem Vorhang wo wir dann alles aufklären konnten. \\\n",
    "Aber ich bin unhöflich. Setzt Euch doch. Darf ich Euch etwas zu trinken anbieten?\"'\n",
    "\n",
    "doc = nlp(test_text_real)\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e478519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "Siona schüttelt lächelnd den Kopf. \"Ich hatte es vorhin nicht erwähnt. Ich komme aus Bjaldorn, falls Euch das etwas sagt.\" Bei der Erinnerung huscht ein kleiner Schatten über ihre Gesichtszüge, doch fängt sie sich schnell wieder. \"Ifirn Kristallpalast ist dort, seit ein paar Jahren ist er auch wieder heiliger Grund und Boden.\" Jetzt lächelt sie schon wieder in Erinnerung an die Eisrose, welche Segen nach Borbarads Schergen zurück brachte. PER\n"
     ]
    }
   ],
   "source": [
    "test_text_real2 = 'Siona schüttelt lächelnd den Kopf. \"Ich hatte es vorhin nicht erwähnt. Ich komme aus Bjaldorn, \\\n",
    "falls Euch das etwas sagt.\" Bei der Erinnerung huscht ein kleiner Schatten über ihre Gesichtszüge, \\\n",
    "doch fängt sie sich schnell wieder. \"Ifirn Kristallpalast ist dort, seit ein paar Jahren ist er auch \\\n",
    "wieder heiliger Grund und Boden.\" Jetzt lächelt sie schon wieder in Erinnerung an die Eisrose, \\\n",
    "welche Segen nach Borbarads Schergen zurück brachte.'\n",
    "\n",
    "doc = nlp(test_text_real2)\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76cbb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "Borbarad reiste nach Gareth und traf auf den Dämon Belhalhar, während er zu Praios betete. PER\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Borbarad reiste nach Gareth und traf auf den Dämon Belhalhar, während er zu Praios betete.\"\n",
    "\n",
    "doc = nlp(test_text)\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70a8f8",
   "metadata": {},
   "source": [
    "---------------------------------- NER + EntityRuler ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395cc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timc/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7577 Muster zum EntityRuler hinzugefügt.\n",
      "Iteration 1 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2724.7185)}\n",
      "Iteration 2 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2413.5)}\n",
      "Iteration 3 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2261.4216)}\n",
      "Iteration 4 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2141.977)}\n",
      "Iteration 5 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2030.1023)}\n",
      "Iteration 6 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1904.1267)}\n",
      "Iteration 7 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1792.431)}\n",
      "Iteration 8 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1687.7426)}\n",
      "Iteration 9 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1604.2323)}\n",
      "Iteration 10 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1538.4286)}\n",
      "Iteration 11 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1487.2383)}\n",
      "Iteration 12 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1442.9498)}\n",
      "Iteration 13 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1400.9174)}\n",
      "Iteration 14 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1361.4255)}\n",
      "Iteration 15 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1317.8024)}\n",
      "Iteration 16 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1273.2345)}\n",
      "Iteration 17 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1222.9381)}\n",
      "Iteration 18 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1176.2838)}\n",
      "Iteration 19 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1134.8018)}\n",
      "Iteration 20 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1107.3881)}\n",
      "Iteration 21 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1095.1523)}\n",
      "Iteration 22 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1088.8317)}\n",
      "Iteration 23 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1074.2136)}\n",
      "Iteration 24 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1055.8727)}\n",
      "Iteration 25 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1034.1433)}\n",
      "Iteration 26 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1013.8319)}\n",
      "Iteration 27 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(987.8934)}\n",
      "Iteration 28 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(950.1579)}\n",
      "Iteration 29 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(904.95435)}\n",
      "Iteration 30 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(858.83026)}\n",
      "Iteration 31 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(805.36536)}\n",
      "Iteration 32 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(745.5154)}\n",
      "Iteration 33 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(673.20496)}\n",
      "Iteration 34 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(592.04877)}\n",
      "Iteration 35 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(517.26746)}\n",
      "Iteration 36 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(444.77377)}\n",
      "Iteration 37 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(373.58267)}\n",
      "Iteration 38 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(303.024)}\n",
      "Iteration 39 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(236.59683)}\n",
      "Iteration 40 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(184.9675)}\n",
      "Iteration 41 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(139.36464)}\n",
      "Iteration 42 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(97.968346)}\n",
      "Iteration 43 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(67.37845)}\n",
      "Iteration 44 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(42.6276)}\n",
      "Iteration 45 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(28.239117)}\n",
      "Iteration 46 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(18.661966)}\n",
      "Iteration 47 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(13.276835)}\n",
      "Iteration 48 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(10.5269575)}\n",
      "Iteration 49 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(8.007319)}\n",
      "Iteration 50 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(6.874046)}\n",
      "Iteration 51 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(5.7303967)}\n",
      "Iteration 52 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(4.243749)}\n",
      "Iteration 53 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.375233)}\n",
      "Iteration 54 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.1489667)}\n",
      "Iteration 55 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.37092742)}\n",
      "Iteration 56 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.13136926)}\n",
      "Iteration 57 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.04392814)}\n",
      "Iteration 58 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.005451615)}\n",
      "Iteration 59 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.0014271638)}\n",
      "Iteration 60 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.0005086703)}\n",
      "Modell gespeichert unter: dsa_ner_model_clean\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DSA NER + EntityRuler Training (Offsets fix)\n",
    "Autor: ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.training.example import Example\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "# 1. Listen laden\n",
    "def load_list(path):\n",
    "    return [line.strip() for line in Path(path).read_text(encoding=\"utf8\").split(\"\\n\") if line.strip()]\n",
    "\n",
    "daemonen = load_list(\"dsa_dnames.txt\")\n",
    "goetter  = load_list(\"dsa_gnames.txt\")\n",
    "personen = load_list(\"dsa_names.txt\")\n",
    "staedte  = load_list(\"dsa_stadtnames.txt\")\n",
    "\n",
    "\n",
    "# 2. Pre-trained Modell\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "\n",
    "# 3. EntityRuler vor NER\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = []\n",
    "for n in daemonen: patterns.append({\"label\": \"DAEMON\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "for n in goetter:  patterns.append({\"label\": \"GOTT\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "for n in personen: patterns.append({\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "for n in staedte:  patterns.append({\"label\": \"STADT\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "ruler.add_patterns(patterns)\n",
    "print(f\"{len(patterns)} Muster zum EntityRuler hinzugefügt.\")\n",
    "\n",
    "\n",
    "# 4. NER Labels\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"DAEMON\")\n",
    "ner.add_label(\"GOTT\")\n",
    "ner.add_label(\"PERSON\")\n",
    "ner.add_label(\"STADT\")\n",
    "\n",
    "\n",
    "# 5. Automatische Trainingsdaten mit char_span Fix\n",
    "import random\n",
    "from spacy.training import Example\n",
    "\n",
    "def generate_training_examples(nlp, num_examples=50):\n",
    "    examples = []\n",
    "\n",
    "    \n",
    "    # Erweiterte Beispiele aus Testtext\n",
    "    erweiterte_sents = [\n",
    "        # Positive Beispiele\n",
    "        (\"Amara lacht bei der Frage.\", [(0, 5, \"PERSON\")]),\n",
    "        (\"In Thorwal bringen wir die Nummer nicht.\", [(3, 9, \"STADT\")]),\n",
    "        (\"Ein tapferer Recke stürmt in die Manege.\", [(3, 18, \"PERSON\")]),\n",
    "        (\"Amara reicht ihre Hand.\", [(0, 5, \"PERSON\")]),\n",
    "        (\"Thorwal ist eine große Stadt.\", [(0, 6, \"STADT\")]),\n",
    "        (\"Ein Zuschauer stürmt auf die Bühne.\", [(3, 11, \"PERSON\")]),\n",
    "        \n",
    "        # Negative Beispiele (keine Entität)\n",
    "        (\"Unterdessen antwortet sie schon.\", []),\n",
    "        (\"Das Publikum reagiert stark.\", []),\n",
    "        (\"Sie geht hinter den Vorhang.\", []),\n",
    "        (\"Rein kommt niemand während der Vorstellung.\", []),\n",
    "        (\"Die Nummer ist eine Provokation.\", []),\n",
    "        (\"Er senkt den Kopf vor ihm.\", []),\n",
    "        (\"Ich komme mit.\", []),\n",
    "        (\"Kann ich das auch tun?\", [])\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Basis-Satztypen\n",
    "    for _ in range(num_examples):\n",
    "        p = random.choice(personen)\n",
    "        s = random.choice(staedte)\n",
    "        g = random.choice(goetter)\n",
    "        d = random.choice(daemonen)\n",
    "\n",
    "        s1 = f\"{p} reiste nach {s}.\"\n",
    "        s2 = f\"{g} sandte einen Dämon namens {d}.\"\n",
    "        s3 = f\"{p} traf {g} in {s}.\"\n",
    "        s4 = f\"Ein Dämon namens {d} erschien vor {p}.\"\n",
    "        s5 = f\"{g} erschien in {s} und sprach zu {p}.\"\n",
    "        s6 = f\"{p} kämpfte gegen den Dämon {d} in {s}.\"\n",
    "        s7 = f\"{g} und {d} waren die Ursache für Chaos in {s}.\"\n",
    "        s8 = f\"{p} erhielt einen Auftrag von {g}, einen Dämon namens {d} zu besiegen.\"\n",
    "        s9 = f\"{d} wurde von {p} in {s} gebannt.\"\n",
    "        s10 = f\"{p}, {g} und {d} trafen sich in {s} zu einem Ritual.\"\n",
    "\n",
    "        sents_and_ents = [\n",
    "            (s1, [(p,\"PERSON\"), (s,\"STADT\")]),\n",
    "            (s2, [(g,\"GOTT\"), (d,\"DAEMON\")]),\n",
    "            (s3, [(p,\"PERSON\"), (g,\"GOTT\"), (s,\"STADT\")]),\n",
    "            (s4, [(d,\"DAEMON\"), (p,\"PERSON\")]),\n",
    "            (s5, [(g,\"GOTT\"), (s,\"STADT\"), (p,\"PERSON\")]),\n",
    "            (s6, [(p,\"PERSON\"), (d,\"DAEMON\"), (s,\"STADT\")]),\n",
    "            (s7, [(g,\"GOTT\"), (d,\"DAEMON\"), (s,\"STADT\")]),\n",
    "            (s8, [(p,\"PERSON\"), (g,\"GOTT\"), (d,\"DAEMON\")]),\n",
    "            (s9, [(d,\"DAEMON\"), (p,\"PERSON\"), (s,\"STADT\")]),\n",
    "            (s10, [(p,\"PERSON\"), (g,\"GOTT\"), (d,\"DAEMON\"), (s,\"STADT\")])\n",
    "        ]\n",
    "\n",
    "        # Alle Basis-Sätze hinzufügen\n",
    "        for sent, ents in sents_and_ents:\n",
    "            doc = nlp.make_doc(sent)\n",
    "            spans = []\n",
    "            for text_val, label in ents:\n",
    "                span = doc.char_span(\n",
    "                    sent.index(text_val),\n",
    "                    sent.index(text_val)+len(text_val),\n",
    "                    label=label,\n",
    "                    alignment_mode=\"contract\"\n",
    "                )\n",
    "                if span:\n",
    "                    spans.append(span)\n",
    "            doc.ents = spans\n",
    "            ex = Example.from_dict(doc, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in spans]})\n",
    "            examples.append(ex)\n",
    "\n",
    "    # Erweiterte Sätze hinzufügen \n",
    "    for sent, ents in erweiterte_sents:\n",
    "        doc = nlp.make_doc(sent)\n",
    "        spans = []\n",
    "        for start, end, label in ents:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "        doc.ents = spans\n",
    "        ex = Example.from_dict(doc, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in spans]})\n",
    "        examples.append(ex)\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "# Training-Daten generieren\n",
    "TRAIN_DATA = generate_training_examples(nlp, num_examples=50)  # 500 Sätze\n",
    "\n",
    "# 6. Training\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "for i in range(60):\n",
    "    losses = {}\n",
    "    nlp.update(TRAIN_DATA, sgd=optimizer, losses=losses)\n",
    "    print(f\"Iteration {i+1} – Losses: {losses}\")\n",
    "\n",
    "\n",
    "# 7. Modell speichern\n",
    "OUTPUT_MODEL = \"dsa_ner_model_clean\"\n",
    "nlp.to_disk(OUTPUT_MODEL)\n",
    "print(f\"Modell gespeichert unter: {OUTPUT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decf8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "Amara PERSON\n",
      "Ja GOTT\n",
      "Thorwal STADT\n",
      "Wilde PERSON\n",
      "Naja GOTT\n",
      "Euch GOTT\n",
      "Darf GOTT\n"
     ]
    }
   ],
   "source": [
    "test_text_real = 'Bei der Frage ob ihre Haut wirklich so dunkel sei muss Amara lachen. \"Ja, die ist schon immer so.\" \\\n",
    "antwortet sie und reicht der Geweihten dann ihre Hand, damit diese sich die dunkle Haut näher \\\n",
    "ansehen kann. Offenbar ist sie durchaus daran gewöhnt solche Fragen zu bekommen. \\\n",
    "Unterdessen antwortet sie aber schon auf die übrigen Bemerkungen. \\\n",
    "\"In Thorwal bringen wir die Nummer natürlich nicht. Aber Ihr habt natürlich recht. Es ist eine \\\n",
    "Provokation. Früher hatten wir das auch nicht drin. Es hat sich eigentlich eher zufällig so ergeben. \\\n",
    "Das Publikum reagiert eben einfach stärker auf eine kaum zu bändigende Wilde als auf eine ... \\\n",
    "Naja, normale Artistin die zufällig dunkle Haut hat. \\\n",
    "Ein mal hatten wir einen tapferen Recken im Publikum, der noch während der Vorstellung in die \\\n",
    "Manege gestürmt ist um mich zu befreien. Aber wir sind bei der Nummer ja hinter Gittern und ehe \\\n",
    "er da rein gekommen ist war ich schon hinter dem Vorhang wo wir dann alles aufklären konnten. \\\n",
    "Aber ich bin unhöflich. Setzt Euch doch. Darf ich Euch etwas zu trinken anbieten?\"'\n",
    "\n",
    "doc = nlp(test_text_real)\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69eb668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "Siona PERSON\n",
      "Bjaldorn PERSON\n",
      "Ifirn PERSON\n",
      "Kristallpalast PERSON\n",
      "Eisrose PERSON\n",
      "Borbarads STADT\n"
     ]
    }
   ],
   "source": [
    "test_text_real2 = 'Siona schüttelt lächelnd den Kopf. \"Ich hatte es vorhin nicht erwähnt. Ich komme aus Bjaldorn, \\\n",
    "falls Euch das etwas sagt.\" Bei der Erinnerung huscht ein kleiner Schatten über ihre Gesichtszüge, \\\n",
    "doch fängt sie sich schnell wieder. \"Ifirn Kristallpalast ist dort, seit ein paar Jahren ist er auch \\\n",
    "wieder heiliger Grund und Boden.\" Jetzt lächelt sie schon wieder in Erinnerung an die Eisrose, \\\n",
    "welche Segen nach Borbarads Schergen zurück brachte.'\n",
    "\n",
    "doc = nlp(test_text_real2)\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c45bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "Borbarad PERSON\n",
      "Gareth PERSON\n",
      "Dämon PERSON\n",
      "Belhalhar DAEMON\n",
      "Praios PERSON\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Borbarad reiste nach Gareth und traf auf den Dämon Belhalhar, während er zu Praios betete.\"\n",
    "\n",
    "doc = nlp(test_text)\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3375ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travia PERSON\n",
      "Praiodane PERSON\n",
      "Nordhag STADT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"In der Kapelle der Travia sprach die Geweihte mit Praiodane aus Nordhag.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c657fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borbarad PER\n",
      "Gareth LOC\n",
      "Belhalhar PER\n",
      "Praios PER\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "doc = nlp(test_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974fe0f",
   "metadata": {},
   "source": [
    "Speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"dsa_nlp_hybrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fcb89",
   "metadata": {},
   "source": [
    "Laden: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"dsa_nlp_hybrid\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rpg-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
