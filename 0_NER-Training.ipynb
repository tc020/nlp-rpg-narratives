{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dac3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa_dnames.txt       # Dämonennamen\n",
    "dsa_gnames.txt       # Gottheiten\n",
    "dsa_names.txt        # Aventurische Personennamen\n",
    "dsa_stadtnames.txt   # Städte Aventuriens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b6459",
   "metadata": {},
   "source": [
    "NER Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "575c2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timc/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Borbarad beschwor einen Zant, bevor er nach Gareth...\" with entities \"[(0, 8, 'PERSON'), (25, 29, 'DAEMON'), (47, 53, 'S...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/timc/Workspace/nlp-rpg-narratives/nlp-rpg-venv/lib/python3.13/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Praios sandte eine Vision nach Alveran.\" with entities \"[(0, 6, 'GOTT'), (29, 36, 'STADT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3.3892465)}\n",
      "Iteration 2 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3.1181347)}\n",
      "Iteration 3 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.7268949)}\n",
      "Iteration 4 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.4281201)}\n",
      "Iteration 5 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.2720454)}\n",
      "Iteration 6 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.2525506)}\n",
      "Iteration 7 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.0454106)}\n",
      "Iteration 8 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.1010563)}\n",
      "Iteration 9 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.9682595)}\n",
      "Iteration 10 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.8356689)}\n",
      "Iteration 11 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.7498648)}\n",
      "Iteration 12 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.6072273)}\n",
      "Iteration 13 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.4488435)}\n",
      "Iteration 14 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.2655911)}\n",
      "Iteration 15 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1.1325285)}\n",
      "Iteration 16 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.80598795)}\n",
      "Iteration 17 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.85973734)}\n",
      "Iteration 18 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.5907108)}\n",
      "Iteration 19 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.4830042)}\n",
      "Iteration 20 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.3627265)}\n",
      "Iteration 21 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.28119642)}\n",
      "Iteration 22 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.18816148)}\n",
      "Iteration 23 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.12126941)}\n",
      "Iteration 24 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.07135854)}\n",
      "Iteration 25 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.031724066)}\n",
      "Iteration 26 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.014304182)}\n",
      "Iteration 27 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.006324596)}\n",
      "Iteration 28 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.0028033687)}\n",
      "Iteration 29 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.0039125467)}\n",
      "Iteration 30 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(0.00013812055)}\n",
      "Modell gespeichert unter: dsa_ner_model\n",
      "Gareth PERSON\n",
      "Borbarad PERSON\n",
      "Dämon PERSON\n",
      "Belhalhar DAEMON\n",
      "Praios PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Listen laden\n",
    "# -------------------------------\n",
    "def load_list(path):\n",
    "    return [line.strip() for line in Path(path).read_text(encoding=\"utf8\").split(\"\\n\") if line.strip()]\n",
    "\n",
    "daemonen = load_list(\"dsa_dnames.txt\")\n",
    "goetter = load_list(\"dsa_gnames.txt\")\n",
    "personen = load_list(\"dsa_names.txt\")\n",
    "staedte = load_list(\"dsa_stadtnames.txt\")\n",
    "\n",
    "# -------------------------------\n",
    "# Pretrained Modell laden (enthält tok2vec)\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # enthält bereits tok2vec → kein KeyError\n",
    "\n",
    "# -------------------------------\n",
    "# NER Labels hinzufügen\n",
    "# -------------------------------\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"DAEMON\")\n",
    "ner.add_label(\"GOTT\")\n",
    "ner.add_label(\"PERSON\")\n",
    "ner.add_label(\"STADT\")\n",
    "\n",
    "# -------------------------------\n",
    "# EntityRuler hinzufügen\n",
    "# -------------------------------\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\", config={\"overwrite_ents\": True})\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = []\n",
    "for n in daemonen:\n",
    "    patterns.append({\"label\": \"DAEMON\", \"pattern\": n})\n",
    "for n in goetter:\n",
    "    patterns.append({\"label\": \"GOTT\", \"pattern\": n})\n",
    "for n in personen:\n",
    "    patterns.append({\"label\": \"PERSON\", \"pattern\": n})\n",
    "for n in staedte:\n",
    "    patterns.append({\"label\": \"STADT\", \"pattern\": n})\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# -------------------------------\n",
    "# Trainingsdaten\n",
    "# -------------------------------\n",
    "TRAIN_DATA = [\n",
    "    (\"Borbarad beschwor einen Zant, bevor er nach Gareth zog.\",\n",
    "     {\"entities\": [(0,8,\"PERSON\"), (25,29,\"DAEMON\"), (47,53,\"STADT\")]}),\n",
    "    (\"Praios sandte eine Vision nach Alveran.\",\n",
    "     {\"entities\": [(0,6,\"GOTT\"), (29,36,\"STADT\")]}),\n",
    "]\n",
    "\n",
    "# Training\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "for i in range(30):\n",
    "    losses = {}\n",
    "    examples = []\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        examples.append(Example.from_dict(doc, annotations))\n",
    "    nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "    print(f\"Iteration {i+1} – Losses: {losses}\")\n",
    "    \n",
    "# -------------------------------\n",
    "# Speichern\n",
    "# -------------------------------\n",
    "OUTPUT_MODEL = \"dsa_ner_model\"\n",
    "nlp.to_disk(OUTPUT_MODEL)\n",
    "print(f\"Modell gespeichert unter: {OUTPUT_MODEL}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Test\n",
    "# -------------------------------\n",
    "doc = nlp(\"In Gareth traf Borbarad auf den Dämon Belhalhar und betete zu Praios.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0395cc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7577 Muster zum EntityRuler hinzugefügt.\n",
      "Iteration 1 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(5421.4253)}\n",
      "Iteration 2 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(5036.749)}\n",
      "Iteration 3 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(4717.8755)}\n",
      "Iteration 4 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(4438.7183)}\n",
      "Iteration 5 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(4176.8486)}\n",
      "Iteration 6 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3930.6633)}\n",
      "Iteration 7 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3707.0312)}\n",
      "Iteration 8 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3503.385)}\n",
      "Iteration 9 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3339.789)}\n",
      "Iteration 10 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3218.0188)}\n",
      "Iteration 11 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3130.6729)}\n",
      "Iteration 12 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3063.3882)}\n",
      "Iteration 13 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(3006.7148)}\n",
      "Iteration 14 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2947.8345)}\n",
      "Iteration 15 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2882.6426)}\n",
      "Iteration 16 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2810.139)}\n",
      "Iteration 17 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2732.1326)}\n",
      "Iteration 18 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2648.5374)}\n",
      "Iteration 19 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2557.2412)}\n",
      "Iteration 20 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2469.1792)}\n",
      "Iteration 21 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2387.0977)}\n",
      "Iteration 22 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2320.3408)}\n",
      "Iteration 23 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2271.7007)}\n",
      "Iteration 24 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2229.6736)}\n",
      "Iteration 25 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2185.8906)}\n",
      "Iteration 26 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2137.565)}\n",
      "Iteration 27 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2076.7766)}\n",
      "Iteration 28 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2000.0857)}\n",
      "Iteration 29 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1910.9362)}\n",
      "Iteration 30 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1787.9258)}\n",
      "Iteration 31 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1647.2423)}\n",
      "Iteration 32 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1505.5769)}\n",
      "Iteration 33 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1353.0167)}\n",
      "Iteration 34 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1200.3379)}\n",
      "Iteration 35 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(1053.34)}\n",
      "Iteration 36 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(904.4093)}\n",
      "Iteration 37 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(797.09753)}\n",
      "Iteration 38 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(692.03766)}\n",
      "Iteration 39 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(606.64545)}\n",
      "Iteration 40 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(515.6992)}\n",
      "Iteration 41 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(432.4013)}\n",
      "Iteration 42 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(345.5348)}\n",
      "Iteration 43 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(265.76648)}\n",
      "Iteration 44 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(186.9672)}\n",
      "Iteration 45 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(131.67752)}\n",
      "Iteration 46 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(84.7284)}\n",
      "Iteration 47 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(55.39595)}\n",
      "Iteration 48 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(36.705017)}\n",
      "Iteration 49 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(24.899426)}\n",
      "Iteration 50 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(19.500086)}\n",
      "Iteration 51 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(16.606266)}\n",
      "Iteration 52 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(15.374677)}\n",
      "Iteration 53 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(13.466094)}\n",
      "Iteration 54 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(12.851921)}\n",
      "Iteration 55 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(11.842374)}\n",
      "Iteration 56 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(10.816007)}\n",
      "Iteration 57 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(8.9240465)}\n",
      "Iteration 58 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(6.2193775)}\n",
      "Iteration 59 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.9872441)}\n",
      "Iteration 60 – Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'lemmatizer': 0.0, 'ner': np.float32(2.5140314)}\n",
      "Modell gespeichert unter: dsa_ner_model_clean\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DSA NER + EntityRuler Training (Offsets fix)\n",
    "Autor: ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.training.example import Example\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Listen laden\n",
    "# -------------------------------\n",
    "def load_list(path):\n",
    "    return [line.strip() for line in Path(path).read_text(encoding=\"utf8\").split(\"\\n\") if line.strip()]\n",
    "\n",
    "daemonen = load_list(\"dsa_dnames.txt\")\n",
    "goetter  = load_list(\"dsa_gnames.txt\")\n",
    "personen = load_list(\"dsa_names.txt\")\n",
    "staedte  = load_list(\"dsa_stadtnames.txt\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Pre-trained Modell\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. EntityRuler vor NER\n",
    "# -------------------------------\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = []\n",
    "for n in daemonen: patterns.append({\"label\": \"DAEMON\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "for n in goetter:  patterns.append({\"label\": \"GOTT\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "for n in personen: patterns.append({\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "for n in staedte:  patterns.append({\"label\": \"STADT\", \"pattern\": [{\"LOWER\": n.lower()}]})\n",
    "ruler.add_patterns(patterns)\n",
    "print(f\"{len(patterns)} Muster zum EntityRuler hinzugefügt.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. NER Labels\n",
    "# -------------------------------\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"DAEMON\")\n",
    "ner.add_label(\"GOTT\")\n",
    "ner.add_label(\"PERSON\")\n",
    "ner.add_label(\"STADT\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Automatische Trainingsdaten mit char_span Fix\n",
    "# -------------------------------\n",
    "import random\n",
    "from spacy.training import Example\n",
    "\n",
    "def generate_training_examples(nlp, num_examples=50):\n",
    "    examples = []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Erweiterte Beispiele aus Testtext\n",
    "    # -------------------------------\n",
    "    erweiterte_sents = [\n",
    "        # Positive Beispiele\n",
    "        (\"Amara lacht bei der Frage.\", [(0, 5, \"PERSON\")]),\n",
    "        (\"In Thorwal bringen wir die Nummer nicht.\", [(3, 9, \"STADT\")]),\n",
    "        (\"Ein tapferer Recke stürmt in die Manege.\", [(3, 18, \"PERSON\")]),\n",
    "        (\"Amara reicht ihre Hand.\", [(0, 5, \"PERSON\")]),\n",
    "        (\"Thorwal ist eine große Stadt.\", [(0, 6, \"STADT\")]),\n",
    "        (\"Ein Zuschauer stürmt auf die Bühne.\", [(3, 11, \"PERSON\")]),\n",
    "        \n",
    "        # Negative Beispiele (keine Entität)\n",
    "        (\"Unterdessen antwortet sie schon.\", []),\n",
    "        (\"Das Publikum reagiert stark.\", []),\n",
    "        (\"Sie geht hinter den Vorhang.\", []),\n",
    "        (\"Rein kommt niemand während der Vorstellung.\", []),\n",
    "        (\"Die Nummer ist eine Provokation.\", [])\n",
    "    ]\n",
    "\n",
    "    # -------------------------------\n",
    "    # Basis-Satztypen\n",
    "    # -------------------------------\n",
    "    for _ in range(num_examples):\n",
    "        p = random.choice(personen)\n",
    "        s = random.choice(staedte)\n",
    "        g = random.choice(goetter)\n",
    "        d = random.choice(daemonen)\n",
    "\n",
    "        s1 = f\"{p} reiste nach {s}.\"\n",
    "        s2 = f\"{g} sandte einen Dämon namens {d}.\"\n",
    "        s3 = f\"{p} traf {g} in {s}.\"\n",
    "        s4 = f\"Ein Dämon namens {d} erschien vor {p}.\"\n",
    "        s5 = f\"{g} erschien in {s} und sprach zu {p}.\"\n",
    "        s6 = f\"{p} kämpfte gegen den Dämon {d} in {s}.\"\n",
    "        s7 = f\"{g} und {d} waren die Ursache für Chaos in {s}.\"\n",
    "        s8 = f\"{p} erhielt einen Auftrag von {g}, einen Dämon namens {d} zu besiegen.\"\n",
    "        s9 = f\"{d} wurde von {p} in {s} gebannt.\"\n",
    "        s10 = f\"{p}, {g} und {d} trafen sich in {s} zu einem Ritual.\"\n",
    "\n",
    "        sents_and_ents = [\n",
    "            (s1, [(p,\"PERSON\"), (s,\"STADT\")]),\n",
    "            (s2, [(g,\"GOTT\"), (d,\"DAEMON\")]),\n",
    "            (s3, [(p,\"PERSON\"), (g,\"GOTT\"), (s,\"STADT\")]),\n",
    "            (s4, [(d,\"DAEMON\"), (p,\"PERSON\")]),\n",
    "            (s5, [(g,\"GOTT\"), (s,\"STADT\"), (p,\"PERSON\")]),\n",
    "            (s6, [(p,\"PERSON\"), (d,\"DAEMON\"), (s,\"STADT\")]),\n",
    "            (s7, [(g,\"GOTT\"), (d,\"DAEMON\"), (s,\"STADT\")]),\n",
    "            (s8, [(p,\"PERSON\"), (g,\"GOTT\"), (d,\"DAEMON\")]),\n",
    "            (s9, [(d,\"DAEMON\"), (p,\"PERSON\"), (s,\"STADT\")]),\n",
    "            (s10, [(p,\"PERSON\"), (g,\"GOTT\"), (d,\"DAEMON\"), (s,\"STADT\")])\n",
    "        ]\n",
    "\n",
    "        # Alle Basis-Sätze hinzufügen\n",
    "        for sent, ents in sents_and_ents:\n",
    "            doc = nlp.make_doc(sent)\n",
    "            spans = []\n",
    "            for text_val, label in ents:\n",
    "                span = doc.char_span(\n",
    "                    sent.index(text_val),\n",
    "                    sent.index(text_val)+len(text_val),\n",
    "                    label=label,\n",
    "                    alignment_mode=\"contract\"\n",
    "                )\n",
    "                if span:\n",
    "                    spans.append(span)\n",
    "            doc.ents = spans\n",
    "            ex = Example.from_dict(doc, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in spans]})\n",
    "            examples.append(ex)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Erweiterte Sätze hinzufügen\n",
    "    # -------------------------------\n",
    "    for sent, ents in erweiterte_sents:\n",
    "        doc = nlp.make_doc(sent)\n",
    "        spans = []\n",
    "        for start, end, label in ents:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "        doc.ents = spans\n",
    "        ex = Example.from_dict(doc, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in spans]})\n",
    "        examples.append(ex)\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training-Daten generieren\n",
    "# -------------------------------\n",
    "TRAIN_DATA = generate_training_examples(nlp, num_examples=100)  # 500 Sätze\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Training\n",
    "# -------------------------------\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "for i in range(60):\n",
    "    losses = {}\n",
    "    nlp.update(TRAIN_DATA, sgd=optimizer, losses=losses)\n",
    "    print(f\"Iteration {i+1} – Losses: {losses}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Modell speichern\n",
    "# -------------------------------\n",
    "OUTPUT_MODEL = \"dsa_ner_model_clean\"\n",
    "nlp.to_disk(OUTPUT_MODEL)\n",
    "print(f\"Modell gespeichert unter: {OUTPUT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9decf8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "amara PERSON\n",
      "thorwal STADT\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 8. Test\n",
    "# -------------------------------\n",
    "\n",
    "test_text_real = 'Bei der Frage ob ihre Haut wirklich so dunkel sei muss Amara lachen. \"Ja, die ist schon immer so.\" \\\n",
    "antwortet sie und reicht der Geweihten dann ihre Hand, damit diese sich die dunkle Haut näher \\\n",
    "ansehen kann. Offenbar ist sie durchaus daran gewöhnt solche Fragen zu bekommen. \\\n",
    "Unterdessen antwortet sie aber schon auf die übrigen Bemerkungen. \\\n",
    "\"In Thorwal bringen wir die Nummer natürlich nicht. Aber Ihr habt natürlich recht. Es ist eine \\\n",
    "Provokation. Früher hatten wir das auch nicht drin. Es hat sich eigentlich eher zufällig so ergeben. \\\n",
    "Das Publikum reagiert eben einfach stärker auf eine kaum zu bändigende Wilde als auf eine ... \\\n",
    "Naja, normale Artistin die zufällig dunkle Haut hat. \\\n",
    "Ein mal hatten wir einen tapferen Recken im Publikum, der noch während der Vorstellung in die \\\n",
    "Manege gestürmt ist um mich zu befreien. Aber wir sind bei der Nummer ja hinter Gittern und ehe \\\n",
    "er da rein gekommen ist war ich schon hinter dem Vorhang wo wir dann alles aufklären konnten. \\\n",
    "Aber ich bin unhöflich. Setzt Euch doch. Darf ich Euch etwas zu trinken anbieten?\"'\n",
    "\n",
    "doc = nlp(test_text_real.lower())\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d69eb668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "siona PERSON\n",
      "kopf PERSON\n",
      "komme PERSON\n",
      "bjaldorn PERSON\n",
      "huscht LOC\n",
      "ifirn PERSON\n",
      "kristallpalast PERSON\n",
      "borbarads STADT\n"
     ]
    }
   ],
   "source": [
    "test_text_real2 = 'Siona schüttelt lächelnd den Kopf. \"Ich hatte es vorhin nicht erwähnt. Ich komme aus Bjaldorn, \\\n",
    "falls Euch das etwas sagt.\" Bei der Erinnerung huscht ein kleiner Schatten über ihre Gesichtszüge, \\\n",
    "doch fängt sie sich schnell wieder. \"Ifirn Kristallpalast ist dort, seit ein paar Jahren ist er auch \\\n",
    "wieder heiliger Grund und Boden.\" Jetzt lächelt sie schon wieder in Erinnerung an die Eisrose, \\\n",
    "welche Segen nach Borbarads Schergen zurück brachte.'\n",
    "\n",
    "doc = nlp(test_text_real2.lower())\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c45bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testausgabe:\n",
      "\n",
      "borbarad PERSON\n",
      "gareth PERSON\n",
      "dämon PERSON\n",
      "belhalhar DAEMON\n",
      "praios PERSON\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Borbarad reiste nach Gareth und traf auf den Dämon Belhalhar, während er zu Praios betete.\"\n",
    "\n",
    "doc = nlp(test_text.lower())\n",
    "print(\"\\nTestausgabe:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3375ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travia PERSON\n",
      "Praiodane PERSON\n",
      "Nordhag STADT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"In der Kapelle der Travia sprach die Geweihte mit Praiodane aus Nordhag.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974fe0f",
   "metadata": {},
   "source": [
    "Speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"dsa_nlp_hybrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fcb89",
   "metadata": {},
   "source": [
    "Laden: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"dsa_nlp_hybrid\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rpg-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
