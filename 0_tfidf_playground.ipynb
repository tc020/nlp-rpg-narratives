{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "from pipeline_merge_pdf import process_pdf_folder\n",
    "import os\n",
    "import time\n",
    "\n",
    "#Ordner, in dem PDFs liegen\n",
    "pdf_folder = \"../data/DSA/Der_blaue_Bruder\"\n",
    "merged_pdf_path = os.path.join(pdf_folder, \"_combined.pdf\")\n",
    "\n",
    "#Prüfen ob eine merged-Datei bereits vorhanden ist. Falls nicht, diese erzeugen\n",
    "if os.path.exists(merged_pdf_path):\n",
    "    print(\"Die Datei _combined.pdf existiert bereits unter folgendem Pfad:\",merged_pdf_path)\n",
    "    print(\"Lade die Datei lokal..\\n\")\n",
    "    time.sleep(0.5)\n",
    "    nlp = spacy.blank(\"de\")\n",
    "    layout = spaCyLayout(nlp)\n",
    "    doc = layout(merged_pdf_path)\n",
    "else:\n",
    "    print(\"Die Datei _combined.pdf existiert noch nicht. Merge-Pipeline wird durchgeführt..\")\n",
    "    #Pipeline ausführen\n",
    "    doc = process_pdf_folder(pdf_folder)\n",
    "\n",
    "#spaCy-Doc\n",
    "print(\"\\nAnzahl Layout-Spans:\", len(doc.spans[\"layout\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_preproc_pdf import process_doc_into_posts\n",
    "\n",
    "#Posts extrahieren + bereinigen\n",
    "posts = process_doc_into_posts(doc)\n",
    "\n",
    "#Posts ausgeben\n",
    "for i, p in enumerate(posts):\n",
    "    print(f\"--- Post {i+1} ---\\n{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d6e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b710c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81462224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installationshinweise (falls nötig)\n",
    "# !pip install nltk seaborn matplotlib scikit-learn\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deutsche Stopwords laden\n",
    "german_stopwords = stopwords.words('german')\n",
    "\n",
    "# TF-IDF Vectorizer mit Stopword-Entfernung\n",
    "vectorizer = TfidfVectorizer(stop_words=german_stopwords)\n",
    "X = vectorizer.fit_transform(posts)\n",
    "\n",
    "# In DataFrame umwandeln für bessere Lesbarkeit\n",
    "df_tfidf = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Post {i+1}\" for i in range(len(posts))]\n",
    ")\n",
    "\n",
    "df_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 1. TF-IDF pro Post berechnen\n",
    "# ------------------------------\n",
    "german_stopwords = stopwords.words('german') #Stopwords aus NLTK laden\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=german_stopwords)\n",
    "X = vectorizer.fit_transform(posts)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Relevante Wörter pro Post\n",
    "# ------------------------------\n",
    "def top_words_per_post(tfidf_vector, features, top_k=5):\n",
    "    sorted_idx = tfidf_vector.toarray()[0].argsort()[::-1]\n",
    "    top_idx = sorted_idx[:top_k]\n",
    "    return features[top_idx], tfidf_vector.toarray()[0][top_idx]\n",
    "\n",
    "top_words = []\n",
    "for i in range(len(posts)):\n",
    "    words, scores = top_words_per_post(X[i], feature_names)\n",
    "    top_words.append(words)\n",
    "\n",
    "# Alle Top-Wörter je Post als Liste flach machen\n",
    "flat_words = np.unique(np.concatenate(top_words))\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Clustering der Wörter\n",
    "# ------------------------------\n",
    "\n",
    "# Wir erstellen TF-IDF Vektoren NUR für die Top-Wörter\n",
    "word_vectors = vectorizer.transform(flat_words)\n",
    "\n",
    "# KMeans Cluster\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "# Cluster → Wörter\n",
    "clusters = {}\n",
    "for word, label in zip(flat_words, labels):\n",
    "    clusters.setdefault(label, []).append(word)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Cluster benennen\n",
    "# (Einfachste Variante: häufigstes Wort im Cluster)\n",
    "# ------------------------------\n",
    "cluster_labels = {}\n",
    "for cid, words in clusters.items():\n",
    "    cluster_labels[cid] = words[0]   # Placeholder\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Häufigkeit: welches Cluster wie oft pro Post?\n",
    "# ------------------------------\n",
    "cluster_counts = []\n",
    "\n",
    "for words in top_words:\n",
    "    counts = {cid: 0 for cid in clusters.keys()}\n",
    "    for w in words:\n",
    "        # Finde den Cluster dieses Wortes\n",
    "        cid = labels[list(flat_words).index(w)]\n",
    "        counts[cid] += 1\n",
    "    cluster_counts.append(counts)\n",
    "\n",
    "df_cluster = pd.DataFrame(cluster_counts)\n",
    "df_cluster.index = [f\"Post {i+1}\" for i in range(len(posts))]\n",
    "\n",
    "print(\"Cluster -> Wörter:\")\n",
    "for cid, words in clusters.items():\n",
    "    print(f\"{cid}: {words}\")\n",
    "\n",
    "print(\"\\nHäufigkeiten je Post:\")\n",
    "print(df_cluster)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rpg-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
